<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>phd | Rodrigo Lira</title>
    <link>https://rodrigoclira.github.io/tag/phd/</link>
      <atom:link href="https://rodrigoclira.github.io/tag/phd/index.xml" rel="self" type="application/rss+xml" />
    <description>phd</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>pt</language><copyright>Rodrigo Lira · 2024</copyright><lastBuildDate>Thu, 22 Jun 2023 10:18:25 -0300</lastBuildDate>
    <image>
      <url>https://rodrigoclira.github.io/media/icon_huf2b89bb3aeddaf27bedd36282c847631_693_512x512_fill_lanczos_center_3.png</url>
      <title>phd</title>
      <link>https://rodrigoclira.github.io/tag/phd/</link>
    </image>
    
    <item>
      <title>Material sobre Reinforcement Learning</title>
      <link>https://rodrigoclira.github.io/post/2023/rl-material/</link>
      <pubDate>Thu, 22 Jun 2023 10:18:25 -0300</pubDate>
      <guid>https://rodrigoclira.github.io/post/2023/rl-material/</guid>
      <description>&lt;p&gt;Eu estou usando Aprendizam Por Reforço (do inglês, Reinforcement Learning) na minha tese de doutorado e no processo de aprendizado eu acabei encontrando muitos materias interessantes na internet. Resolvi fazer uma listagem deles para deixar público e também me ajudar em consultas futuras.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Para quem estiver procurando conhecimento teórico, eu sugiro os livros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement Learning: An Introduction&lt;/a&gt; de Sutton e Barto&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.ualberta.ca/~szepesva/rlbook.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Algorithms of Reinforcement Learning&lt;/a&gt; de Szepesvari&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Para quem quiser conhecer um pouco da teoria ao mesmo tempo que desenvolve projetos práticos com Python:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/learn/deep-rl-course/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL Course&lt;/a&gt; do Hugginfaces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://applied-rl-course.netlify.app/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Reinforcement Learning&lt;/a&gt; com RLLib&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://courses.dibya.online/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Real World Deep RL*&lt;/a&gt; do Dibya.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://spinningup.openai.com/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open AI Spinning up in Deep RL&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;* Há também o curso &lt;a href=&#34;https://courses.dibya.online/p/fastdeeprl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast RL&lt;/a&gt; que eu não realizei.&lt;/p&gt;
&lt;p&gt;Disciplinas em instituições:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rail.eecs.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 285 @ UC Berkeley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs234/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS234 @ Stanford&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Lecture Series @ DeepMind and UCL &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Reinfocement Learning with David Silver @ Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Publicação no Latin America Conference on Computational Intelligence (LA-CCI 2021)</title>
      <link>https://rodrigoclira.github.io/post/2021/lacci2021/</link>
      <pubDate>Sun, 26 Sep 2021 23:57:14 -0300</pubDate>
      <guid>https://rodrigoclira.github.io/post/2021/lacci2021/</guid>
      <description>&lt;p&gt;O artigo &amp;ldquo;Modelling the Social Interactions in Grey Wolf Optimizer&amp;rdquo; foi aceito no &lt;a href=&#34;https://fbln.me/lacci/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LA-CCI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Autores: &lt;strong&gt;Rodrigo C. Lira&lt;/strong&gt;, Mariana Macedo, Hugo Valadares Siqueira. Ronaldo Menezes, Carmelo Bastos-Filho&lt;/p&gt;
&lt;p&gt;Abstract:
&amp;ldquo;Swarm  Intelligence  has  been  successfully  used  forsolving high-dimensional and multimodal optimization problems.However, the wide range of swarm-based techniques, operators,and  parameters  requires  prior  knowledge  before  applying  themto  real-world  problems.  Because  of  this,  we  have  been  study-ing  the  meso-level  characteristics  that  emerge  from  the  socialinteractions  within  the  swarm  to  understand  each  swarm-basedtechnique’s  unique  characteristics.  In  this  paper,  we  model  andstudy the interaction network of the Grey Wolf Optimizer (GWO)to  capture  its  social  behaviour.  We  used  Portrait  divergence  tocompare  the  similarity  between  network  structures  over  exper-iments,  simulations  and  iterations  of  the  GWO.  We  also  usedKullback divergence to compare the probability distributions ofthe  network  flows  varying  over  experiments,  simulations  anditerations  of  the  GWO.  Furthermore,  we  discovered  we  couldidentify  the  GWO  convergence  using  the  interaction  networkapproach.  Comparing  different  simulations,  we  found  that  thewolves  communicate  using  a  stable  network  structure  but  notnecessarily  a  stable  network  flow  indicating  variance  in  thenumber  of  highly  influential  wolves.  We  also  point  out  patternsfound in GWO that appears to be similar to other swarm-basedalgorithms  (GPSO  and  FSS).&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
